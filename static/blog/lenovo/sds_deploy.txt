### KVM 环境准备：

- Note：
鉴于物理服务器的物理磁盘的IO限制。
为了提高搭建sds1.1的系统响应速度。请将该环境中所有的虚拟磁盘，包括系统盘和数据盘。尽量分散到物理服务器的多个物理磁盘上。
可以使用命令 iostat –d –k 5 查看系统磁盘io使用情况。
* * *
### 网络准备：
新建3个网络：(名称任意取，区别于其他人，网段任选，未被其他网络使用)
	sds-admin:192.168.1.0/24
	sds-public: 192.168.2.0/24
	sds-cluster: 192.168.0.0/24
* * *
### VM 准备：
数量4：1(controller) + 3(node）
OS：centos7.3_1611
## Controller配置：
- 网络：
	2个网卡，2个网络。
	Controller 只需要配置2个网络：admin , public.
但为了方便网络验证，也可以三个都配置。
- IP 配置：
	192.168.1.2
	192.168.2.2
- /etc/hosts配置：
```
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 controller
::1	localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.2 api.inte.lenovo.com
```
在第一行，后面添加controller。并添加第三行，IP为controller管理网段IP。
- 主机名：
	将hostname修改为controller
	hostnamectl set-hostname controller
- 关闭防火墙:
	systemctl stop firewalld
- 关闭selinux：
	setenforce 0
并修改/etc/sysconfig/selinux:
SELINUX=disabled
## node 配置：
- 网络： 
	3个网卡，3个网络都需要配置
- IP：
node1)
	192.168.0.3
	192.168.1.3
	192.168.2.3
(node2)
	192.168.0.4
	192.168.1.4
	192.168.2.4
(node3)
192.168.0.5
	192.168.1.5
	192.168.2.5
- disk配置：
	每个node要求有3个disk，请自行添加。尽量落在不同物理磁盘上。
	每个磁盘添加时，请选择scsi模式。
	如果已添加，可在VM控制窗口中磁盘的advanced options中重新修改
	其他格式的磁盘，sds1.1会认不到。
## 其他配置：
为方便之后的工作，配置所需要的其他非必须选项，如controller 到node 的ssh免密登录。Controller的大网IP等。

大网IP的配置，可以使用71服务器上/root/Desktop 目录下的两个脚本来探测已使用和未使用的10.240.217网段的大网 IP。直接执行即可。
```
ip_scan_in_using.sh 
ip_scan_not_using.sh
```
* * *
### sds1.1 安装：
安装前，请给所有4台虚机做一个快照，方便以后将环境推到重来。
	
登录controller并下载最新的release包：
```
cd Downloads
wget --no-check-certificate https://10.240.205.130/pub/thinkcloud_sds//dev1.1/daily/deployment-standalone-daily_20170825_27.tar.gz
tar –zxvf *.tar.gz
cd deployment
./standalone-setup.sh install
```
创建集群并添加主机：
```
#执行命令前，先要执行环境变量配置。
source ~/localrc
#该步骤会在命令行立即返回，但会在界面等待约10到20分钟的时间。
cephmgmtclient create-cluster --name cluster1 --addr shanghai
cephmgmtclient create-server --id 1 --name node-1 --publicip 192.168.2.3 --clusterip 192.168.0.3 --server_user root --server_pass passw0rd --parent_bucket 1
cephmgmtclient create-server --id 1 --name node-2 --publicip 192.168.2.4 --clusterip 192.168.0.4 --server_user root --server_pass passw0rd --parent_bucket 1
cephmgmtclient create-server --id 1 --name node-3 --publicip 192.168.2.5 --clusterip 192.168.0.5 --server_user root --server_pass passw0rd --parent_bucket 1
```
```
#如果需要，创建第二个集群，并添加主机（需要再创建3个VM）:
cephmgmtclient create-cluster --name cluster2 --addr shanghai
cephmgmtclient create-server --id 2 --name node-4 --publicip 192.168.2.6 --clusterip 192.168.0.6 --server_user root --server_pass passw0rd --parent_bucket 2
cephmgmtclient create-server --id 2 --name node-5 --publicip 192.168.2.7 --clusterip 192.168.0.7 --server_user root --server_pass passw0rd --parent_bucket 2
cephmgmtclient create-server --id 2 --name node-6 --publicip 192.168.2.8 --clusterip 192.168.0.8 --server_user root --server_pass passw0rd --parent_bucket 2 
```
添加主机之后，查看页面，有可能会出现添加失败的情况，需要重置controller数据库，再次添加即可成功。
```
./standalone-setup.sh initdb
```
	再次执行上面添加服务器的命令。


所有主机添加完毕并在页面显示成功后，回到集群页面。点击执行部署即可，并等待完成。
目前阶段，ceph页面可能会显示HEALTH_ERROR 或HEALTH_WARNING，之后会有一段时间ceph用来实现自平衡。才能显示为HEALTH_OK。


